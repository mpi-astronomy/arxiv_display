{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92bcb855",
   "metadata": {},
   "source": [
    "# MPIA Arxiv on Deck 2\n",
    "\n",
    "Contains the steps to produce the paper extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0d6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from PIL import Image \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from arxiv_on_deck_2.arxiv2 import (get_new_papers, \n",
    "                                    get_paper_from_identifier,\n",
    "                                    retrieve_document_source, \n",
    "                                    get_markdown_badge)\n",
    "from arxiv_on_deck_2 import (latex, \n",
    "                             mpia,\n",
    "                             highlight_authors_in_list)\n",
    "\n",
    "# Sometimes images are really big\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22aa9d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful definitions.\n",
    "\n",
    "class AffiliationWarning(UserWarning):\n",
    "    pass\n",
    "\n",
    "class AffiliationError(RuntimeError):\n",
    "    pass\n",
    "\n",
    "def validation(source: str):\n",
    "    \"\"\"Raises error paper during parsing of source file\n",
    "    \n",
    "    Allows checks before parsing TeX code.\n",
    "    \n",
    "    Raises AffiliationWarning\n",
    "    \"\"\"\n",
    "    check = mpia.affiliation_verifications(source, verbose=True)\n",
    "    if check is not True:\n",
    "        raise AffiliationError(\"mpia.affiliation_verifications: \" + check)\n",
    "\n",
    "        \n",
    "warnings.simplefilter('always', AffiliationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14622700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional CSS to consider. \n",
    "# TODO: Current into each exported file. \n",
    "#       This should be set once into the webpages directly.\n",
    "#       With only the class/id definitions in the .md files.\n",
    "\n",
    "debug_html = \"\"\"\n",
    "<style>\n",
    "#wrap{ overflow:auto; }\n",
    "#fig1{ background:yellow; width:100%; float:left; padding:5px;  }\n",
    "#fig2{ background:red; width:50%; float:left; clear:left; padding:5px;  }\n",
    "#fig3{ background:green; width:50%; float:left; padding:5px;   }\n",
    ".macros{ background:yellow; visibility:visible;}\n",
    "h1 {margin: 0 0 0 0;}\n",
    "mark {background-color:#fff3b6;}\n",
    "img {object-fit:contain; max-height:250px; display:inline-block; text-align: center;}\n",
    "</style>\n",
    "\"\"\" \n",
    "html = \"\"\"\n",
    "<style>\n",
    "#wrap{ overflow:auto; }\n",
    "#fig1{ width:100%; float:left; padding: 5px;  }\n",
    "#fig2{ width:50%; float:left; clear:left; padding: 5px;  }\n",
    "#fig3{ width:50%; float:left; padding: 5px;  }\n",
    ".macros{ visibility:hidden; height:0px; }\n",
    "h1 {margin: 0em 0 0 0;}\n",
    "mark {background-color:#fff3b6;}\n",
    "img {object-fit:contain; max-height:250px; display:inline-block; text-align: center;}\n",
    "</style>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd6310",
   "metadata": {},
   "source": [
    "## get list of arxiv paper candidates\n",
    "\n",
    "We use the MPIA mitarbeiter list webpage from mpia.de to get author names\n",
    "We then get all new papers from Arxiv and match authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2645e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arxiv has 36 new papers today\n",
      "          10 with possible author matches\n"
     ]
    }
   ],
   "source": [
    "# get list from MPIA website\n",
    "# it automatically filters identified non-scientists :func:`mpia.filter_non_scientists`\n",
    "mpia_authors = mpia.get_mpia_mitarbeiter_list()\n",
    "new_papers = get_new_papers()\n",
    "# add manual references\n",
    "add_paper_refs = []\n",
    "new_papers.extend([get_paper_from_identifier(k) for k in add_paper_refs])\n",
    "\n",
    "# select only papers with matching author names and highlight authors\n",
    "hl_list = [k[0] for k in mpia_authors]\n",
    "\n",
    "candidates = []\n",
    "for paperk in new_papers:\n",
    "    hl_authors = highlight_authors_in_list(paperk['authors'], hl_list)\n",
    "    matches = [(hl, orig) for hl, orig in zip(hl_authors, paperk['authors']) if 'mark' in hl]\n",
    "    paperk['authors'] = hl_authors\n",
    "    if matches:\n",
    "        candidates.append(paperk)\n",
    "print(\"\"\"Arxiv has {0:,d} new papers today\"\"\".format(len(new_papers)))        \n",
    "print(\"\"\"          {0:,d} with possible author matches\"\"\".format(len(candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543b34a",
   "metadata": {},
   "source": [
    "# Parse sources and generate relevant outputs\n",
    "\n",
    "From the candidates, we do the following steps:\n",
    "* get their tarball from ArXiv (and extract data)\n",
    "* find the main .tex file: find one with \\documentclass{...} (sometimes it's non trivial)\n",
    "* Check affiliations with :func:`validation`, which uses :func:`mpia.affiliation_verifications`\n",
    "* If passing the affiliations: we parse the .tex source\n",
    "   * inject sub-documents into the main (flatten the main document)\n",
    "   * parse structure, extract information (title, abstract, authors, figures...)\n",
    "   * handles `\\graphicspath` if provided\n",
    "* Generate the .md document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9576b79e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff0113e84e64d388b3fde64784edbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving document from  https://arxiv.org/e-print/2204.02410\n",
      "extracting tarball to tmp_2204.02410... done.\n",
      "Retrieving document from  https://arxiv.org/e-print/2204.02416\n",
      "extracting tarball to tmp_2204.02416... done.\n",
      "Retrieving document from  https://arxiv.org/e-print/2204.02501\n",
      "extracting tarball to tmp_2204.02501... done.\n",
      "Retrieving document from  https://arxiv.org/e-print/2204.02523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: LatexWarning: Multiple tex files.\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: LatexWarning: Found documentclass in tmp_2204.02501/mnras_template.tex\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting tarball to tmp_2204.02523... done.\n",
      "Retrieving document from  https://arxiv.org/e-print/2204.02540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: LatexWarning: Multiple tex files.\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: LatexWarning: Found documentclass in tmp_2204.02523/aassymbols.tex\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting tarball to tmp_2204.02540... done.\n",
      "Retrieving document from  https://arxiv.org/e-print/2204.02716\n",
      "extracting tarball to tmp_2204.02716... done.\n",
      "Retrieving document from  https://arxiv.org/e-print/2204.02719\n",
      "extracting tarball to tmp_2204.02719... done.\n",
      "Retrieving document from  https://arxiv.org/e-print/2204.02780\n",
      "extracting tarball to tmp_2204.02780... done.\n",
      "Retrieving document from  https://arxiv.org/e-print/2204.02796\n",
      "extracting tarball to tmp_2204.02796... done.\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "failed = []\n",
    "for paper in tqdm(candidates[:-1]):\n",
    "    paper_id = paper['identifier'].lower().replace('arxiv:', '')\n",
    "    \n",
    "    folder = f'tmp_{paper_id}'\n",
    "\n",
    "    try:\n",
    "        if not os.path.isdir(folder):\n",
    "            folder = retrieve_document_source(f\"{paper_id}\", f'tmp_{paper_id}')\n",
    "        \n",
    "        try:\n",
    "            doc = latex.LatexDocument(folder, validation=validation)    \n",
    "        except AffiliationError as affilerror:\n",
    "            msg = f\"ArXiv:{paper_id:s} is not an MPIA paper... \" + str(affilerror)\n",
    "            failed.append((paper, \"affiliation error: \" + str(affilerror) ))\n",
    "            continue\n",
    "        \n",
    "        # Hack because sometimes author parsing does not work well\n",
    "        if (len(doc.authors) != len(paper['authors'])):\n",
    "            doc._authors = paper['authors']\n",
    "        if (doc.abstract) is None:\n",
    "            doc.abstract = paper['abstract']\n",
    "            \n",
    "        doc.comment = get_markdown_badge(paper_id) + \" _\" + paper['comments'] + \"_\"\n",
    "        doc.highlight_authors_in_list(hl_list)\n",
    "\n",
    "        full_md = doc.generate_markdown_text()\n",
    "        documents.append((paper_id, full_md))\n",
    "    except Exception as e:\n",
    "        warnings.warn(latex.LatexWarning(f\"{paper_id:s} did not run properly\\n\" +\n",
    "                                         str(e)\n",
    "                                        ))\n",
    "        failed.append((paper, \"latex error \" + str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505a25c",
   "metadata": {},
   "source": [
    "### Export the logs\n",
    "\n",
    "Throughout, we also keep track of the logs per paper. see `logs-{today date}.md` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d733828a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Successful papers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Failed papers"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02410-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02410) | **How Cosmic Rays Mediate the Evolution of the Interstellar Medium**  |\n",
       "|| Christine M. Simpson, et al. -- incl., <mark>Rowan Smith</mark> |\n",
       "|*Comments*| *27 pages, 21 Figures, submitted to MNRAS, comments welcome*|\n",
       "|**Abstract**| We explore the impact of diffusive cosmic rays (CRs) on the evolution of the interstellar medium (ISM) under varying assumptions of supernova explosion environment. In practice, we systematically vary the relative fractions of supernovae (SN) occurring in star-forming high-density gas and those occurring in random locations decoupled from star-forming gas to account for SN from run-away stars or explosions in regions that have been cleared by prior SN, stellar winds, or radiation. We explore various mixed models by adjusting these fractions relative to each other. We find that in the simple system of a periodic stratified gas layer the ISM structure will evolve to one of two solutions: a \"peak driving\" state where warm gas is volume filling or a \"thermal runaway\" state where hot gas is volume filling. CR pressure and transport are important factors that strongly influence the solution state the ISM reaches and have the ability to flip the ISM between solutions. Observable signatures such as gamma ray emission and HI gas are explored. We find that gamma ray luminosity from pion decay is largely consistent with observations for a range of model parameters. The thickness of the HI gas layer may be too compact, however, this may be due to a large cold neutral fraction of midplane gas. The volume fraction of hot gas evolves to stable states in both solutions, but neither settles to a Milky Way-like configuration, suggesting that additional physics which is omitted here (e.g. a cosmological circum-galactic medium, radiation transport, or spectrally resolved and spatially varying CR transport) may be required. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: '69117' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02416-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02416) | **Unexplored outflows in nearby low luminosity AGNs: the case of NGC 1052**  |\n",
       "|| S. Cazzoli, et al. -- incl., <mark>L. Hermosa Muñoz</mark>, <mark>L. Hernández-García</mark>, <mark>C. Ramos Almeida</mark> |\n",
       "|*Comments*| *A&A accepted 04/04/2022, 31 pages, 12 figures and 3 appendixes*|\n",
       "|**Abstract**| Outflows play a central role in galaxy evolution shaping the properties of galaxies. Understanding outflows and their effects in low luminosity AGNs, such as LINERs, is essential (e.g. they are a numerous AGN population in the local Universe). We obtained VLT/MUSE and GTC/MEGARA optical IFS-data for NGC1052, the prototypical LINER. The stars are distributed in a dynamically hot disc, with a centrally peaked velocity dispersion map and large observed velocity amplitudes. The ionised gas, probed by the primary component is detected up to $\\sim$30arcsec ($\\sim$3.3 kpc) mostly in the polar direction with blue and red velocities ($\\mid$V$\\mid$$<$250 km/s). The velocity dispersion map shows a notable enhancement ($\\sigma$$>$90 km/s) crossing the galaxy along the major axis of rotation in the central 10arcsec. The secondary component has a bipolar morphology, velocity dispersion larger than 150 km/s and velocities up to 660 km/s. A third component is detected but not spatially resolved. The maps of the NaD absorption indicate optically thick neutral gas with a velocity field consistent with a slow rotating disc ($\\Delta$V = 77$\\pm$12 km/s) but the velocity dispersion map is off-centred without any counterpart in the flux map. We found evidence of an ionised gas outflow with mass of 1.6$\\pm$0.6 $\\times$ 10$^{5}$ Msun, and mass rate of 0.4$\\pm$0.2 Msun/yr. The outflow is propagating in a cocoon of gas with enhanced turbulence and might be triggering the onset of kpc-scale buoyant bubbles (polar emission). Taking into account the energy and kinetic power of the outflow (1.3$\\pm$0.9 $\\times$ 10$^{53}$ erg and 8.8$\\pm$3.5 $\\times$ 10$^{40}$ erg/s, respectively) as well as its alignment with both the jet and the cocoon, and that the gas is collisionally ionised, we consider that the outflow is jet-powered, although some contribution from the AGN is possible. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: 'Heidelberg' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02501-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02501) | **Pulsating hydrogen-deficient white dwarfs and pre-white dwarfs observed  with TESS -- IV. Discovery of two new GW Vir stars: TIC0403800675 and  TIC1989122424**  |\n",
       "|| Murat Uzundag, et al. -- incl., <mark>Leandro G. Althaus</mark> |\n",
       "|*Comments*| *8 Pages. arXiv admin note: text overlap with arXiv:2108.11093*|\n",
       "|**Abstract**| We present two new GW Vir-type pulsating white dwarf stars, TIC\\,0403800675 (WD\\,J115727.68-280349.64) and TIC\\,1989122424 (WD J211738.38-552801.18) discovered in the Transiting Exoplanet Survey Satellite (TESS) photometric data. For both stars, the TESS light curves reveal the presence of oscillations with periods in a narrow range between 400 and 410\\,s, which are associated with typical gravity ($g$)-modes. Follow-up ground-based spectroscopy shows that both stars have similar effective temperature ($T_\\mathrm{eff} = 110,000 \\pm 10,000$\\,K) and surface gravity ($\\log g = 7.5 \\pm 0.5$), but different He/C composition (mass fractions): He\\,=\\,0.75 and C\\,=\\,0.25 for TIC\\,0403800675, and He\\,=\\,0.50 and C\\,=\\,0.50 for TIC\\,1989122424. By performing a fit to their spectral energy distributions, we found for both stars radii and luminosities of $R=0.019\\pm0.002\\,R_\\odot$ and $\\log(L/L_\\odot)=1.68^{+0.15}_{-0.24}$, respectively. By employing evolutionary tracks of PG~1159 stars, we find the masses of both stars to be $0.56\\pm0.18 M_{\\odot}$ from the $\\log g$-$T_\\mathrm{eff}$ diagram and $0.60^{+0.11}_{-0.09} M_{\\odot}$ from the Hertzsprung Russell diagram. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: 'Heidelberg' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02523-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02523) | **LADUMA: Discovery of a luminous OH megamaser at $z > 0.5$**  |\n",
       "|| Marcin Glowacki, et al. -- incl., <mark>Bradley Frank</mark>, <mark>Julia Healy</mark>, <mark>Patricia Henning</mark>, <mark>Kelley M. Hess</mark>, <mark>Ian Heywood</mark>, <mark>John P. Hughes</mark>, <mark>Martin Meyer</mark> |\n",
       "|*Comments*| *10 pages, 4 figures. Accepted to ApJ Letters*|\n",
       "|**Abstract**| In the local Universe, OH megamasers (OHMs) are detected almost exclusively in infrared-luminous galaxies, with a prevalence that increases with IR luminosity, suggesting that they trace gas-rich galaxy mergers. Given the proximity of the rest frequencies of OH and the hyperfine transition of neutral atomic hydrogen (HI), radio surveys to probe the cosmic evolution of HI in galaxies also offer exciting prospects for exploiting OHMs to probe the cosmic history of gas-rich mergers. Using observations for the Looking At the Distant Universe with the MeerKAT Array (LADUMA) deep HI survey, we report the first untargeted detection of an OHM at $z > 0.5$, LADUMA J033046.20$-$275518.1 (nicknamed \"Nkalakatha\"). The host system, WISEA J033046.26$-$275518.3, is an infrared-luminous radio galaxy whose optical redshift $z \\approx 0.52$ confirms the MeerKAT emission line detection as OH at a redshift $z_{\\rm OH} = 0.5225 \\pm 0.0001$ rather than HI at lower redshift. The detected spectral line has 18.4$\\sigma$ peak significance, a width of $459 \\pm 59\\,{\\rm km\\,s^{-1}}$, and an integrated luminosity of $(6.31 \\pm 0.18\\,{\\rm [statistical]}\\,\\pm 0.31\\,{\\rm [systematic]}) \\times 10^3\\,L_\\odot$, placing it among the most luminous OHMs known. The galaxy's far-infrared luminosity $L_{\\rm FIR} = (1.576 \\pm 0.013) \\times 10^{12}\\,L_\\odot$ marks it as an ultra-luminous infrared galaxy; its ratio of OH and infrared luminosities is similar to those for lower-redshift OHMs. A comparison between optical and OH redshifts offers a slight indication of an OH outflow. This detection represents the first step towards a systematic exploitation of OHMs as a tracer of galaxy growth at high redshifts. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: 'Heidelberg' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02540-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02540) | **Refinement of the convex shape model and tumbling spin state of (99942)  Apophis using the 2020-2021 apparition data**  |\n",
       "|| <mark>H.-J. Lee</mark>, et al. -- incl., <mark>S. Greenstreet</mark>, <mark>R. Lees</mark> |\n",
       "|*Comments*| *14 pages, 5 figures; Accepted for publication on Astronomy & Astrophysics*|\n",
       "|**Abstract**| Context. The close approach of the near-Earth asteroid (99942) Apophis to Earth in 2029 will provide a unique opportunity to examine how the physical properties of the asteroid could be changed due to the Earth's gravitational perturbation. As a result, the Republic of Korea is planning a rendezvous mission to Apophis. Aims. Our aim was to use photometric data from the apparitions in 2020-2021 to refine the shape model and spin state of Apophis. Methods. Using thirty-six 1 to 2-m class ground-based telescopes and the Transiting Exoplanet Survey Satellite, we performed a photometric observation campaign throughout the 2020-2021 apparition. The convex shape model and spin state were refined using the light-curve inversion method. Results. According to our best-fit model, Apophis is rotating in a short axis mode with rotation and precession periods of 264.178 hours and 27.38547 hours, respectively. The angular momentum vector orientation of Apophis was found as (275$^\\circ$, -85$^\\circ$) in the ecliptic coordinate system. The ratio of the dynamic moments of inertia of this asteroid was fitted to $I_a:I_b:I_c=0.64:0.97:1$, which corresponds to an elongated prolate ellipsoid. These findings regarding the spin state and shape model could be used to not only design the space mission scenario but also investigate the impact of the Earth's tidal force during close encounters. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: 'Heidelberg' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02716-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02716) | **A Novel Cloud-Based Framework for Standardised Simulations in the Latin  American Giant Observatory (LAGO)**  |\n",
       "|| <mark>Antonio Juan Rubio-Montero</mark>, et al. -- incl., <mark>Hernán Asorey</mark> |\n",
       "|*Comments*| *10 pages, 3 figures, Invited Talk at the Winter Simulation Conference WSC2021, Phoenix, AZ, USA*|\n",
       "|**Abstract**| LAGO, the Latin American Giant Observatory, is an extended cosmic ray observatory, consisting of a wide network of water Cherenkov detectors located in 10 countries. With different altitudes and geomagnetic rigidity cutoffs, their geographic distribution, combined with the new electronics for control, atmospheric sensing and data acquisition, allows the realisation of diverse astrophysics studies at a regional scale. It is an observatory designed, built and operated by the LAGO Collaboration, a non-centralised alliance of 30 institutions from 11 countries. While LAGO has access to different computational frameworks, it lacks standardised computational mechanisms to fully grasp its cooperative approach. The European Commission is fostering initiatives aligned to LAGO objectives, especially to enable Open Science and its long-term sustainability. This work introduces the adaptation of LAGO to this paradigm within the EOSC-Synergy project, focusing on the simulations of the expected astrophysical signatures at detectors deployed at the LAGO sites around the World. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: 'Heidelberg' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02719-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02719) | **A study of the F-giant star $θ$ Scorpii A: a post-merger rapid  rotator?**  |\n",
       "|| <mark>Fiona Lewis</mark>, et al. -- incl., <mark>Floor van Leeuwen</mark> |\n",
       "|*Comments*| *12 pages, 10 figures, Accepted by MNRAS*|\n",
       "|**Abstract**| We report high-precision observations of the linear polarization of the F1$\\,$III star $\\theta$ Scorpii. The polarization has a wavelength dependence of the form expected for a rapid rotator, but with an amplitude several times larger than seen in otherwise similar main-sequence stars. This confirms the expectation that lower-gravity stars should have stronger rotational-polarization signatures as a consequence of the density dependence of the ratio of scattering to absorption opacities. By modelling the polarization, together with additional observational constraints (incorporating a revised analysis of Hipparcos astrometry, which clarifies the system's binary status), we determine a set of precise stellar parameters, including a rotation rate $\\omega\\, (= \\Omega/\\Omega_{\\rm c})\\ge 0.94$, polar gravity $\\log{g_p} = 2.091 ^{+0.042}_{-0.039}$ (dex cgs), mass $3.10 ^{+0.37}_{-0.32}$ solar masses, and luminosity $\\log(L/Lsun) =3.149^{+0.041}_{-0.028}$. These values are incompatible with evolutionary models of single rotating stars, with the star rotating too rapidly for its evolutionary stage, and being undermassive for its luminosity. We conclude that $\\theta$ Sco A is most probably the product of a binary merger. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: 'Heidelberg' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02780-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02780) | **Eliminating Primary Beam Effect in Foreground Subtraction of Neutral  Hydrogen Intensity Mapping Survey with Deep Learning**  |\n",
       "|| Shulei Ni, Yichao Li, Li-Yang Gao, <mark>Xin Zhang</mark> |\n",
       "|*Comments*| *17 pages, 12 figures*|\n",
       "|**Abstract**| In the neutral hydrogen (HI) intensity mapping (IM) survey, the foreground contamination on the cosmological signals is extremely severe, and the systematic effects caused by radio telescopes themselves further aggravate the difficulties in subtracting foreground. In this work, we investigate whether the deep learning method, concretely the 3D U-Net algorithm here, can play a crucial role in foreground subtraction when considering the systematic effect caused by the telescope primary beam. We consider two beam models, i.e., the Gaussian beam model as a simple case and the Cosine beam model as a sophisticated case. To make a comparison, and also a combination, with the U-Net method, we also employ the traditional principal component analysis (PCA) method in the foreground subtraction. We find that, in the case of the Gaussian beam, both the PCA and U-Net methods can effectively clean the foreground, but in the case of the Cosine beam, U-Net performs much better than PCA in cleaning the foreground. In order to show how well the PCA and U-Net methods can recover the HI signals, we also derive the angular power spectra, as well as the 2D power spectrum of HI after performing the foreground subtractions. It is found that, in the case of Gaussian beam, the concordance with the original HI map using U-Net is better than that using PCA by $27.4\\%$, and in the case of Cosine beam, the concordance using U-Net is better than that using PCA by $144.7\\%$. Therefore, the U-Net based foreground subtraction can efficiently eliminate the telescope primary beam effect and shed new light on recovering the HI power spectrum for future HI IM experiments. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: 'Heidelberg' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "|||\n",
       "|---:|:---|\n",
       "| [![arXiv](https://img.shields.io/badge/arXiv-arXiv:2204.02796-b31b1b.svg)](https://arxiv.org/abs/arXiv:2204.02796) | **Molecules in the carbon-rich protoplanetary nebula CRL 2688**  |\n",
       "|| Jian-Jie Qiu, <mark>Yong Zhang</mark>, <mark>Jiang-Shui Zhang</mark>, Jun-ichi Nakashima |\n",
       "|*Comments*| *55 pages, 10 figures, Accepted for publication in ApJS*|\n",
       "|**Abstract**| We present observations of the carbon-rich protoplanetary nebula (PPN) CRL 2688 made with the Institut de Radioastronomie Millimetrique (IRAM) 30 m telescope in the 3mm and 2mm bands. In total, 196 transition lines belonging to 38 molecular species and isotopologues are detected, among which, to our best knowledge, 153 transition lines and 13 species are the first report for this object. Additionally, in order to contribute to future research, we have collected observational data on the molecular lines of CRL 2688 from the literature and compiled them into a single unified catalog. We find that the molecular abundance of CRL 2688 cannot be explained by the standard model of a circumstellar envelope. The implications of metal-bearing molecules on circumstellar chemistry are discussed. |\n",
       "|<p style=\"color:green\"> **ERROR** </p>| <p style=\"color:green\">affiliation error: mpia.affiliation_verifications: 'Heidelberg' keyword not found.</p> |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "today = str(datetime.date.today())\n",
    "logfile = f\"_build/html/logs/log-{today}.md\"\n",
    "\n",
    "\n",
    "with open(logfile, 'w') as logs:\n",
    "    # Success\n",
    "    logs.write(f'# Arxiv on Deck 2: Logs - {today}\\n\\n')\n",
    "    logs.write(\"\"\"* Arxiv had {0:,d} new papers\\n\"\"\".format(len(new_papers)))\n",
    "    logs.write(\"\"\"    * {0:,d} with possible author matches\\n\\n\"\"\".format(len(candidates)))\n",
    "    logs.write(\"## Sucessful papers\\n\\n\")\n",
    "    display(Markdown(\"## Successful papers\"))\n",
    "    success = [k[0] for k in documents]\n",
    "    for candid in candidates:\n",
    "        if candid['identifier'].split(':')[-1] in success:\n",
    "            display(candid)\n",
    "            logs.write(candid.generate_markdown_text() + '\\n\\n')\n",
    "\n",
    "    ## failed\n",
    "    logs.write(\"## Failed papers\\n\\n\")\n",
    "    display(Markdown(\"## Failed papers\"))\n",
    "    failed = sorted(failed, key=lambda x: x[1])\n",
    "    current_reason = \"\"\n",
    "    for paper, reason in failed:\n",
    "        if 'affiliation' in reason:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "        data = Markdown(\n",
    "                paper.generate_markdown_text() + \n",
    "                f'\\n|<p style=\"color:{color:s}\"> **ERROR** </p>| <p style=\"color:{color:s}\">{reason:s}</p> |'\n",
    "               )\n",
    "        if reason != current_reason:\n",
    "            logs.write(f'### {reason:s} \\n\\n')\n",
    "            current_reason = reason\n",
    "        logs.write(data.data + '\\n\\n')\n",
    "        \n",
    "        # only display here the important errors (all in logs)\n",
    "        # if color in ('red',):\n",
    "        display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d20ee",
   "metadata": {},
   "source": [
    "## Export documents\n",
    "\n",
    "We now write the .md files and export relevant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d426aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_markdown_summary(md: str, md_fname:str, directory: str):\n",
    "    \"\"\"Export MD document and associated relevant images\"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    import re\n",
    "\n",
    "    if (os.path.exists(directory) and not os.path.isdir(directory)):\n",
    "        raise RuntimeError(f\"a non-directory file exists with name {directory:s}\")\n",
    "\n",
    "    if (not os.path.exists(directory)):\n",
    "        print(f\"creating directory {directory:s}\")\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    fig_fnames = (re.compile(r'\\[Fig.*\\]\\((.*)\\)').findall(md) + \n",
    "                  re.compile(r'\\<img src=\"([^>\\s]*)\"[^>]*/>').findall(md))\n",
    "    for fname in fig_fnames:\n",
    "        if 'http' in fname:\n",
    "            # No need to copy online figures\n",
    "            continue\n",
    "        destdir = os.path.join(directory, os.path.dirname(fname))\n",
    "        destfname = os.path.join(destdir, os.path.basename(fname))\n",
    "        try:\n",
    "            os.makedirs(destdir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        shutil.copy(fname, destfname)\n",
    "    with open(os.path.join(directory, md_fname), 'w') as fout:\n",
    "        fout.write(md)\n",
    "    print(\"exported in \", os.path.join(directory, md_fname))\n",
    "    [print(\"    + \" + os.path.join(directory,fk)) for fk in fig_fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014d04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_id, md in documents:\n",
    "    export_markdown_summary(md, f\"{paper_id:s}.md\", '_build/html/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087a0a7",
   "metadata": {},
   "source": [
    "## Display the papers\n",
    "\n",
    "Not necessary but allows for a quick check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd25f625",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[display(Markdown(k[1])) for k in documents];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873873a4",
   "metadata": {},
   "source": [
    "# Create HTML index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf665672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14  publications in the last 7 days.\n",
      "\t exports/2204.01758.md\n",
      "\t exports/2204.02109.md\n",
      "\t exports/2204.02017.md\n",
      "\t exports/2204.01824.md\n",
      "\t exports/2203.15811.md\n",
      "\t exports/2203.16735.md\n",
      "\t exports/2203.16504.md\n",
      "\t exports/2203.15822.md\n",
      "\t exports/2204.01245.md\n",
      "\t exports/2204.00793.md\n",
      "\t exports/2204.00342.md\n",
      "\t exports/2203.16959.md\n",
      "\t exports/2203.16856.md\n",
      "\t exports/2203.16734.md\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "from glob import glob\n",
    "\n",
    "files = glob('_build/html/*.md')\n",
    "days = 7\n",
    "now = datetime.today()\n",
    "res = []\n",
    "for fk in files:\n",
    "    stat_result = os.stat(fk).st_ctime\n",
    "    modified = datetime.fromtimestamp(stat_result, tz=timezone.utc).replace(tzinfo=None)\n",
    "    delta = now.today() - modified\n",
    "    if delta <= timedelta(days=days):\n",
    "        res.append((delta.seconds, fk))\n",
    "res = [k[1] for k in sorted(res, key=lambda x:x[0])]\n",
    "npub = len(res)\n",
    "print(len(res), f\" publications in the last {days:d} days.\")\n",
    "[ print('\\t', k) for k in res ];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ca0208",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_carousel(npub=4):\n",
    "    \"\"\" Generate the HTML code for a carousel with `npub` slides \"\"\"\n",
    "    carousel = [\"\"\"<section class=\"carousel\" aria-label=\"Gallery\">\"\"\",\n",
    "                \"\"\"  <ol class=\"carousel__viewport\">\"\"\",\n",
    "    ]\n",
    "    for k in range(1, npub + 1):\n",
    "        prev_ = k - 1\n",
    "        next_ = k + 1\n",
    "        if prev_ <= 0:\n",
    "            prev_ = npub\n",
    "        if next_ > npub:\n",
    "            next_ = 1\n",
    "        text  = f\"\"\"    <li id=\"carousel__slide{k}\" tabindex=\"0\" class=\"carousel__slide\">\\n\"\"\"\n",
    "        text += f\"\"\"       <div class=\"carousel__snapper\">\\n\"\"\"\n",
    "        text += f\"\"\"         <a href=\"#carousel__slide{prev_}\" class=\"carousel__prev\">Go to previous slide</a>\\n\"\"\"\n",
    "        text += f\"\"\"         <a href=\"#carousel__slide{next_}\" class=\"carousel__next\">Go to next slide</a>\\n\"\"\"\n",
    "        text += f\"\"\"         <div id=\"slide{k}_content\" class=\"md_view\" >Content {k}</div>\\n\"\"\"\n",
    "        text += f\"\"\"       </div>\\n\"\"\"\n",
    "        text += f\"\"\"    </li>\"\"\"\n",
    "        carousel.append(text)\n",
    "\n",
    "    carousel.extend([\n",
    "        \"\"\"  </ol>\"\"\",\n",
    "        \"\"\"  <aside class=\"carousel__navigation\">\"\"\",\n",
    "        \"\"\"    <ol class=\"carousel__navigation-list\">\"\"\"])\n",
    "\n",
    "    for k in range(1, npub + 1):\n",
    "        text  = f\"\"\"      <li class=\"carousel__navigation-item\">\\n\"\"\"\n",
    "        text += f\"\"\"        <a href=\"#carousel__slide{k}\" class=\"carousel__navigation-button\">Go to {k}</a>\\n\"\"\"\n",
    "        text += f\"\"\"      </li>\"\"\"\n",
    "        carousel.append(text)\n",
    "    carousel.extend([\"\"\"    </ol>\"\"\", \"\"\"  </aside>\"\"\", \"\"\"</section>\"\"\"])\n",
    "\n",
    "    return '\\n'.join(carousel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6eac5b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "carousel = create_carousel(npub)\n",
    "docs = ', '.join(['\"{0:s}\"'.format(k.split('/')[-1]) for k in res])\n",
    "slides = ', '.join([f'\"slide{k}_content\"' for k in range(1, npub + 1)])\n",
    "\n",
    "script = f\"\"\"\n",
    "const docs = [{docs}]\n",
    "\n",
    "const slides = [{slides}]\n",
    "\"\"\" + \"\"\"\n",
    "async function run() {\n",
    "    for (let i = 0; i < docs.length; i++) {\n",
    "        let file = await fetch(docs[i]);\n",
    "        let text = await file.text()\n",
    "        document.getElementById(slides[i]).innerHTML =\n",
    "            marked.parse(text);\n",
    "    }\n",
    "    hljs.highlightAll();\n",
    "}\n",
    "run()\n",
    "\"\"\"\n",
    "\n",
    "page = f\"\"\"<!doctype html>\n",
    "<html lang=\"en\">\n",
    "\n",
    "<head>\n",
    "  <meta charset=\"utf-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "  <!-- Bootstrap CSS -->\n",
    "  <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\"\n",
    "   integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\">\n",
    "  <!-- highlight.js CSS -->\n",
    "  <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/styles/default.min.css\">\n",
    "  <!-- Mathjax 3 -->\n",
    "  <script type=\"text/javascript\" id=\"MathJax-config\" src=\"mathjax_config.js\"> </script>\n",
    "  <script type=\"text/javascript\" id=\"MathJax-script\" async \n",
    "    src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\">\n",
    "  </script>\n",
    "  <link rel=\"stylesheet\" href=\"index_carousel.css\">\n",
    "  <link rel=\"icon\" type=\"image/x-icon\" href=\"https://www.mpia.de/assets/touch-icon-32x32-a66937bcebc4e8894ebff1f41a366c7c7220fd97a38869ee0f2db65a9f59b6c1.png\">\n",
    "  <title>MPIA Arxiv on deck!</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "  <div id=\"header\"> <img src=\"header_banner.png\" width=\"100%\"></div>\n",
    "  <div id=\"suptitle\"> 7-day archives </div>\n",
    "  <div id=\"info\">\n",
    "    <img src=\"https://pngimg.com/uploads/github/github_PNG58.png\" height=30rem></img>\n",
    "    <a href=https://github.com/mpi-astronomy/arxiv_display style=\"color:black;\">github/mpi-astronomy/arxiv_display</a> \n",
    "  </div>\n",
    "  {carousel:s}\n",
    "</body>\n",
    "\n",
    "<!-- Render Markdown -->\n",
    "\n",
    "<body>\n",
    "  <!-- highlight.js: https://highlightjs.org/download/ -->\n",
    "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.0/highlight.min.js\"></script>\n",
    "  <!-- marked.js -->\n",
    "  <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n",
    "  <script>{script:s}</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "with open(\"_build/html/index_7days.html\", 'w') as fout:\n",
    "    fout.write(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebacbc",
   "metadata": {},
   "source": [
    "# Debugging papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18861263",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Manual Stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jb/twt_8kfx4mj86m9v8jgccs_r5gbl99/T/ipykernel_20397/3613455088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Manual Stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: Manual Stop"
     ]
    }
   ],
   "source": [
    "raise NotImplementedError(\"Manual Stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21842d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from TexSoup import TexSoup\n",
    "import re\n",
    "\n",
    "def bracket_error(source: str):\n",
    "    \"\"\" Find problematic portions of the document \"\"\"\n",
    "    \n",
    "    print(\"len(source)\", len(source))\n",
    "    \n",
    "    # Checking header\n",
    "    begin_doc = next(re.finditer(r'\\\\begin\\{document\\}', doc.source)).span()[1]\n",
    "    header = source[:begin_doc]\n",
    "    text = header + r\"\\n\\end{document}\"\n",
    "\n",
    "    try:\n",
    "        # print(\"Header check... \", end='')\n",
    "        TexSoup(text)\n",
    "        display(Markdown(f\"**[OK]** - Header\"))\n",
    "    except:\n",
    "        raise RuntimeError(\"Error in the header\")\n",
    "        \n",
    "    # Check the text per section until the end.\n",
    "    # Do not stop and try them all.\n",
    "    \n",
    "    problematic_text = []\n",
    "    \n",
    "    sections = ([(0, begin_doc, 'until first section')] + \n",
    "                [(g.span()[0], g.span()[1], g.group()) for g in re.finditer(r'\\\\section\\{.*\\}', source)] +\n",
    "                [(g.span()[0], g.span()[1], g.group()) for g in re.finditer(r'\\\\begin\\{appendix\\}', source)]\n",
    "               )\n",
    "    sections.append([len(source), len(source), 'end'])\n",
    "    \n",
    "    sections = sorted(sections, key=lambda x: x[0])\n",
    "    \n",
    "    prev_pos, prev_name = (0, 'header')\n",
    "    parsed = []\n",
    "    \n",
    "    for span, span_end, name in sections:\n",
    "\n",
    "        if span - prev_pos <= 0:\n",
    "            continue\n",
    "            \n",
    "\n",
    "        text = source[prev_pos:span]\n",
    "        if prev_pos > begin_doc:\n",
    "            text = r\"\\n\\begin{document}\" + text + r\"\\n\\end{document}\"\n",
    "        else:\n",
    "            text = text + r\"\\n\\end{document}\"\n",
    "        try:\n",
    "            #print(f\"{prev_pos}:{prev_name}-->{span}:{name} check... \", end='')\n",
    "            parsed.append(TexSoup(text, tolerance=1))  # allow not ending env\n",
    "            display(Markdown(f\"**[OK]** - *{prev_pos}*:{prev_name} &rarr; *{span}*:{name}\"))\n",
    "            # print(\"ok\")\n",
    "\n",
    "            prev_pos = span\n",
    "            prev_name = name\n",
    "        except:\n",
    "            # print(f\"error between {prev_pos} and {span}\")\n",
    "            display(Markdown(f\"**[ERR]** *{prev_pos}*:{prev_name} &rarr; *{span}*:{name}\"))\n",
    "            problematic_text.append((prev_pos, source[prev_pos:span]))\n",
    "            prev_pos = span\n",
    "            prev_name = name\n",
    "            # raise\n",
    "    return problematic_text, parsed\n",
    "\n",
    "\n",
    "def check_environment(text, offset=0):\n",
    "    \"\"\" Check environment \"\"\"\n",
    "    env = re.compile(r\"\\\\begin\\{(?P<env>.*)\\}(.*)\\\\end\\{(?P=env)\\}\", re.DOTALL)\n",
    "\n",
    "    for match in env.finditer(text):\n",
    "        beg, end = match.span()\n",
    "        beg += offset\n",
    "        end += offset\n",
    "        envname = match.groups()[0]\n",
    "        try:\n",
    "            latex.TexSoup(match.group())\n",
    "        except Exception as e:\n",
    "            display(e)\n",
    "            print(f\"Error in {envname:s} between {beg} and {end}\")\n",
    "            return match.groups()[1], beg, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates[1]['identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ccff3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(latex)\n",
    "which = \"2204.02017\"\n",
    "paper_id = f'{which:s}'\n",
    "folder = f'tmp_{paper_id:s}'\n",
    "doc = latex.LatexDocument(folder)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfcdb2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[check_environment(k) for k in bracket_error(doc.source)]\n",
    "_, _, a = latex.get_content_per_section(doc.source, verbose=True)\n",
    "if not a:\n",
    "    print(\"no issues per section\")\n",
    "for ak in a:\n",
    "    r = check_environment(ak[1], offset=ak[0])\n",
    "    print(r[1], r[2])\n",
    "    print(r[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
